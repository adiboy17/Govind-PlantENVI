{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "animeVAE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y6g2I1aFt8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f99820a-e63d-4b3f-c71b-572ccbed8369"
      },
      "source": [
        "!apt install -y ffmpeg  # for GIF creation\n",
        "!pip install ffmpeg     # P.S needs internet on"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 11 not upgraded.\n",
            "Collecting ffmpeg\n",
            "  Downloading https://files.pythonhosted.org/packages/f0/cc/3b7408b8ecf7c1d20ad480c3eaed7619857bf1054b690226e906fdf14258/ffmpeg-1.4.tar.gz\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-cp36-none-any.whl size=6083 sha256=380c288acc52cecfc0e101b3ba20081bc8b803fe612798281bd6d659283d940e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/68/c3/a05a35f647ba871e5572b9bbfc0b95fd1c6637a2219f959e7a\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5gxbph5F5B9"
      },
      "source": [
        "import torch\n",
        "import torchvision.datasets  as dsets\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import animation\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML, display, FileLink"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9OxxaaqF8rq"
      },
      "source": [
        "batch_size = 32\n",
        "image_size = 64\n",
        "num_epochs = 2\n",
        "torch.cuda.set_device(\"cuda:0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnHT8KlSGTU_"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUpOhhSOGgFT"
      },
      "source": [
        "DATA_DIR = '/content/gdrive/My Drive/aug-image'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwdvu-nRGAjN"
      },
      "source": [
        "dset = dsets.ImageFolder(root=\"/content/gdrive/My Drive/aug-image\",\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(image_size),\n",
        "                               transforms.CenterCrop(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                           ]))\n",
        "# cut the size of the dataset\n",
        "dataset, _ = torch.utils.data.random_split(dset, [len(dset)//2, len(dset)-len(dset)//2])\n",
        "del _\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, drop_last=True, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4faroJ7GklN"
      },
      "source": [
        "real_batch = next(iter(dataloader))\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0], padding=2).cpu(),(1,2,0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuIdmFvjG6yA"
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "\n",
        "class UnFlatten(nn.Module):\n",
        "    def forward(self, input, size=1024):\n",
        "        return input.view(input.size(0), size, 1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaMzV0dbHnyP"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, image_channels=3, h_dim=1024, z_dim=32):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            Flatten()\n",
        "        )\n",
        "        \n",
        "        self.h2mu = nn.Linear(h_dim, z_dim)\n",
        "        self.h2sigma = nn.Linear(h_dim, z_dim)\n",
        "        self.z2h = nn.Linear(z_dim, h_dim)\n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "            UnFlatten(),\n",
        "            nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        \n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        eps = torch.randn(*mu.size())\n",
        "        z = mu + std * eps\n",
        "        return z\n",
        "    \n",
        "    def bottleneck(self, h):\n",
        "        mu = self.h2mu(h)\n",
        "        logvar = self.h2sigma(h)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return z, mu, logvar\n",
        "        \n",
        "    def encode(self, x):\n",
        "        return self.bottleneck(self.encoder(x))[0]\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(self.z2h(z))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        z, mu, logvar = self.bottleneck(h)\n",
        "        z = self.z2h(z)\n",
        "        return self.decoder(z), mu, logvar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmJ6IxLPHt0B"
      },
      "source": [
        "def vae_loss(recon_x, x, mu, logvar) -> float:\n",
        "    BCE = F.binary_cross_entropy(recon_x.view(-1, image_size*image_size*3),\n",
        "                                 x.view(-1, image_size*image_size*3), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw9Zq_q-H0Bb"
      },
      "source": [
        "TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm8fiM_2Hxe6"
      },
      "source": [
        "model = VAE()\n",
        "try:\n",
        "    model.load_state_dict(torch.load('vae.pth'))\n",
        "except:\n",
        "    print(\"Weights not found ):\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo6_3Q6OH5Qt"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    for data, _ in tqdm.tqdm(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = vae_loss(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        \n",
        "    torch.save(model.state_dict(), \"vae.pth\")\n",
        "    print('epoch %d, loss %.4f' % (epoch, train_loss / len(dataset)))\n",
        "model.eval()\n",
        "FileLink(r'vae.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93eQIUfWKWAv"
      },
      "source": [
        "def get(x):\n",
        "    return dataset[x][0].view(1, 3, image_size, image_size)\n",
        "\n",
        "def imshow(img):\n",
        "    pic = np.transpose(img.numpy(), (1,2,0))\n",
        "    plt.axis('off')\n",
        "    return plt.imshow(pic, animated=True)\n",
        "\n",
        "def morph(inputs, steps: int, delay: int):\n",
        "    latent = [model.encode(get(k)).data for k in inputs]\n",
        "    fig = plt.figure()\n",
        "    images = []\n",
        "    for a, b in zip(latent, latent[1:] + [latent[0]]):\n",
        "        for t in np.linspace(0, 1, steps):\n",
        "            c = a*(1-t)+b*t\n",
        "            morphed = model.decode(c).data[0]\n",
        "            images.append([imshow(morphed)])\n",
        "    \n",
        "    ani = animation.ArtistAnimation(fig, images, interval=delay)\n",
        "\n",
        "    display(HTML(ani.to_html5_video()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyjBKYMrKhmt"
      },
      "source": [
        "Visualizations time!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvYcFV8cKbU2"
      },
      "source": [
        "num_images = 30 # amount of images in GIF\n",
        "num_steps = 20 # smoothness of transition between images\n",
        "delay = 30\n",
        "morph(np.random.randint(0, len(dataset), num_images), num_steps, delay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNvrt7cXKl0m"
      },
      "source": [
        "decoded_batch = model.decode(model.encode(real_batch[0]).data).data\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(decoded_batch, padding=2).cpu(),(1,2,0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrRl_UGlV6ks"
      },
      "source": [
        " # Denoising  part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-PSqmS5USpq"
      },
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Reading image from folder where it is stored\n",
        "img = cv2.imread(decoded_batch)\n",
        "\n",
        "\n",
        "# denoising of image saving it into dst image\n",
        "dst = cv2.fastNlMeansDenoisingColored(img, None, 10, 10, 7, 15)\n",
        "\n",
        "# Plotting of source and destination image\n",
        "plt.subplot(121), plt.imshow(img)\n",
        "plt.subplot(122), plt.imshow(dst)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}